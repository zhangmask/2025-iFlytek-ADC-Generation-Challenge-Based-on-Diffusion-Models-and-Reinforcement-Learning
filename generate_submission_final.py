#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆç‰ˆæœ¬çš„ADCæäº¤æ–‡ä»¶ç”Ÿæˆè„šæœ¬

ç‰¹ç‚¹ï¼š
1. å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
2. æ•°æ®éªŒè¯å’Œæ ¼å¼æ£€æŸ¥
3. ä¼˜åŒ–çš„åˆ†å­å¤šæ ·æ€§ç”Ÿæˆ
4. åŸºäºç»Ÿè®¡çš„æ™ºèƒ½é¢„æµ‹
5. è‡ªåŠ¨åŒ–çš„æ–‡ä»¶éªŒè¯å’Œæ‰“åŒ…
"""

import os
import sys
import pandas as pd
import numpy as np
import random
import zipfile
from pathlib import Path
import logging
from typing import List, Dict, Tuple, Optional
from rdkit import Chem
from rdkit.Chem import Descriptors, rdMolDescriptors
from rdkit import DataStructs
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('submission_generation.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class ADCSubmissionGenerator:
    """ADCæäº¤æ–‡ä»¶ç”Ÿæˆå™¨ - æœ€ç»ˆç‰ˆæœ¬"""
    
    def __init__(self, random_seed: int = 42):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            random_seed: éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
        """
        self.train_data = None
        self.test_data = None
        self.random_seed = random_seed
        
        # è®¾ç½®éšæœºç§å­
        random.seed(random_seed)
        np.random.seed(random_seed)
        
        # éªŒè¯é…ç½®
        self.required_train_columns = [
            'index', 'Antibody Light Chain Sequence', 'Antibody Heavy Chain Sequence',
            'Antigen Sequence', 'Payload Isosmiles', 'Linker Isosmiles', 'DAR',
            'C1', 'C2', 'C3', 'C4'
        ]
        
        self.required_test_columns = ['index']
        
        logger.info(f"ADCæäº¤æ–‡ä»¶ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œéšæœºç§å­: {random_seed}")
    
    def validate_data_format(self, df: pd.DataFrame, required_columns: List[str], data_type: str) -> bool:
        """éªŒè¯æ•°æ®æ ¼å¼
        
        Args:
            df: æ•°æ®æ¡†
            required_columns: å¿…éœ€çš„åˆ—å
            data_type: æ•°æ®ç±»å‹ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡
        """
        try:
            # æ£€æŸ¥å¿…éœ€åˆ—
            missing_columns = set(required_columns) - set(df.columns)
            if missing_columns:
                logger.error(f"{data_type}ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
                return False
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if df.empty:
                logger.error(f"{data_type}æ•°æ®ä¸ºç©º")
                return False
            
            # æ£€æŸ¥indexåˆ—
            if 'index' in df.columns:
                if df['index'].isnull().any():
                    logger.error(f"{data_type}çš„indexåˆ—åŒ…å«ç©ºå€¼")
                    return False
                
                if not df['index'].dtype in ['int64', 'int32']:
                    logger.warning(f"{data_type}çš„indexåˆ—ç±»å‹ä¸æ˜¯æ•´æ•°: {df['index'].dtype}")
            
            logger.info(f"{data_type}æ ¼å¼éªŒè¯é€šè¿‡: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
            return True
            
        except Exception as e:
            logger.error(f"{data_type}æ ¼å¼éªŒè¯å¤±è´¥: {e}")
            return False
    
    def load_and_validate_data(self) -> bool:
        """åŠ è½½å¹¶éªŒè¯æ•°æ®
        
        Returns:
            åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹åŠ è½½å’ŒéªŒè¯æ•°æ®...")
            
            # åŠ è½½è®­ç»ƒæ•°æ®
            train_path = Path("train.csv")
            if not train_path.exists():
                logger.error(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_path}")
                return False
            
            self.train_data = pd.read_csv(train_path)
            if not self.validate_data_format(self.train_data, self.required_train_columns, "è®­ç»ƒæ•°æ®"):
                return False
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_path = Path("test.csv")
            if not test_path.exists():
                logger.error(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_path}")
                return False
            
            self.test_data = pd.read_csv(test_path)
            if not self.validate_data_format(self.test_data, self.required_test_columns, "æµ‹è¯•æ•°æ®"):
                return False
            
            # éªŒè¯C1-C4åˆ—çš„å€¼
            for c_col in ['C1', 'C2', 'C3', 'C4']:
                unique_values = self.train_data[c_col].dropna().unique()
                if not set(unique_values).issubset({0, 1}):
                    logger.error(f"è®­ç»ƒæ•°æ®{c_col}åˆ—åŒ…å«éäºŒåˆ†ç±»å€¼: {unique_values}")
                    return False
            
            logger.info("æ•°æ®åŠ è½½å’ŒéªŒè¯å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def calculate_molecular_similarity(self, smiles1: str, smiles2: str) -> float:
        """è®¡ç®—åˆ†å­ç›¸ä¼¼æ€§
        
        Args:
            smiles1: ç¬¬ä¸€ä¸ªåˆ†å­çš„SMILES
            smiles2: ç¬¬äºŒä¸ªåˆ†å­çš„SMILES
            
        Returns:
            Tanimotoç›¸ä¼¼æ€§å€¼ (0-1)
        """
        try:
            mol1 = Chem.MolFromSmiles(smiles1)
            mol2 = Chem.MolFromSmiles(smiles2)
            
            if mol1 is None or mol2 is None:
                return 0.0
            
            fp1 = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol1, 2, nBits=2048)
            fp2 = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol2, 2, nBits=2048)
            
            return DataStructs.TanimotoSimilarity(fp1, fp2)
            
        except Exception as e:
            logger.warning(f"è®¡ç®—åˆ†å­ç›¸ä¼¼æ€§å¤±è´¥: {e}")
            return 0.0
    
    def generate_diverse_molecules(self, molecule_type: str, target_count: int, 
                                 similarity_threshold: float = 0.8) -> List[str]:
        """ç”Ÿæˆå¤šæ ·æ€§åˆ†å­
        
        Args:
            molecule_type: åˆ†å­ç±»å‹ ('Payload' æˆ– 'Linker')
            target_count: ç›®æ ‡æ•°é‡
            similarity_threshold: ç›¸ä¼¼æ€§é˜ˆå€¼
            
        Returns:
            ç”Ÿæˆçš„åˆ†å­SMILESåˆ—è¡¨
        """
        try:
            column_name = f"{molecule_type} Isosmiles"
            logger.info(f"ç”Ÿæˆ {target_count} ä¸ªå¤šæ ·æ€§{molecule_type}åˆ†å­...")
            
            # è·å–è®­ç»ƒé›†ä¸­çš„å”¯ä¸€åˆ†å­
            unique_molecules = self.train_data[column_name].dropna().unique().tolist()
            logger.info(f"è®­ç»ƒé›†ä¸­æœ‰ {len(unique_molecules)} ä¸ªå”¯ä¸€{molecule_type}")
            
            if len(unique_molecules) == 0:
                logger.error(f"è®­ç»ƒé›†ä¸­æ²¡æœ‰æœ‰æ•ˆçš„{molecule_type}åˆ†å­")
                return []
            
            generated_molecules = []
            
            # é¦–å…ˆæ·»åŠ æ‰€æœ‰å”¯ä¸€åˆ†å­
            for mol in unique_molecules:
                if len(generated_molecules) < target_count:
                    generated_molecules.append(mol)
            
            # å¦‚æœéœ€è¦æ›´å¤šåˆ†å­ï¼Œä½¿ç”¨åŠ æƒéšæœºé€‰æ‹©
            attempt_count = 0
            max_attempts = target_count * 10  # é˜²æ­¢æ— é™å¾ªç¯
            
            while len(generated_molecules) < target_count and attempt_count < max_attempts:
                attempt_count += 1
                
                # éšæœºé€‰æ‹©ä¸€ä¸ªåˆ†å­
                candidate = random.choice(unique_molecules)
                
                # æ£€æŸ¥å¤šæ ·æ€§
                is_diverse = True
                if len(generated_molecules) > 0:
                    # åªæ£€æŸ¥æœ€è¿‘æ·»åŠ çš„å‡ ä¸ªåˆ†å­ä»¥æé«˜æ•ˆç‡
                    check_count = min(20, len(generated_molecules))
                    recent_molecules = generated_molecules[-check_count:]
                    
                    for existing_mol in recent_molecules:
                        similarity = self.calculate_molecular_similarity(candidate, existing_mol)
                        if similarity > similarity_threshold:
                            is_diverse = False
                            break
                
                if is_diverse:
                    generated_molecules.append(candidate)
                    if len(generated_molecules) % 100 == 0:
                        logger.info(f"å·²ç”Ÿæˆ {len(generated_molecules)} ä¸ª{molecule_type}åˆ†å­")
            
            # å¦‚æœä»ç„¶ä¸å¤Ÿï¼Œç›´æ¥éšæœºå¡«å……
            while len(generated_molecules) < target_count:
                generated_molecules.append(random.choice(unique_molecules))
            
            # è®¡ç®—å¤šæ ·æ€§å¾—åˆ†
            diversity_score = self.calculate_diversity_score(generated_molecules[:100])  # é‡‡æ ·è®¡ç®—
            logger.info(f"{molecule_type}åˆ†å­ç”Ÿæˆå®Œæˆ: {len(generated_molecules)} ä¸ª, å¤šæ ·æ€§å¾—åˆ†: {diversity_score:.4f}")
            
            return generated_molecules[:target_count]
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆ{molecule_type}åˆ†å­å¤±è´¥: {e}")
            return []
    
    def calculate_diversity_score(self, molecules: List[str], sample_size: int = 50) -> float:
        """è®¡ç®—åˆ†å­å¤šæ ·æ€§å¾—åˆ†
        
        Args:
            molecules: åˆ†å­SMILESåˆ—è¡¨
            sample_size: é‡‡æ ·å¤§å°ï¼ˆç”¨äºå¤§æ•°æ®é›†ï¼‰
            
        Returns:
            å¤šæ ·æ€§å¾—åˆ† (0-1ï¼Œè¶Šé«˜è¶Šå¤šæ ·)
        """
        try:
            if len(molecules) < 2:
                return 0.0
            
            # é‡‡æ ·ä»¥æé«˜è®¡ç®—æ•ˆç‡
            if len(molecules) > sample_size:
                sampled_molecules = random.sample(molecules, sample_size)
            else:
                sampled_molecules = molecules
            
            similarities = []
            for i in range(len(sampled_molecules)):
                for j in range(i+1, len(sampled_molecules)):
                    sim = self.calculate_molecular_similarity(sampled_molecules[i], sampled_molecules[j])
                    similarities.append(sim)
            
            if not similarities:
                return 0.0
            
            avg_similarity = np.mean(similarities)
            diversity_score = 1.0 - avg_similarity
            
            return max(0.0, min(1.0, diversity_score))
            
        except Exception as e:
            logger.warning(f"è®¡ç®—å¤šæ ·æ€§å¾—åˆ†å¤±è´¥: {e}")
            return 0.0
    
    def predict_c_values_statistical(self, test_data: pd.DataFrame) -> np.ndarray:
        """åŸºäºç»Ÿè®¡çš„C1-C4å€¼é¢„æµ‹
        
        Args:
            test_data: æµ‹è¯•æ•°æ®
            
        Returns:
            é¢„æµ‹çš„C1-C4å€¼æ•°ç»„
        """
        try:
            logger.info("å¼€å§‹åŸºäºç»Ÿè®¡çš„C1-C4å€¼é¢„æµ‹...")
            
            n_samples = len(test_data)
            
            # è®¡ç®—è®­ç»ƒæ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
            c_stats = {}
            for c_col in ['C1', 'C2', 'C3', 'C4']:
                c_stats[c_col] = {
                    'mean': self.train_data[c_col].mean(),
                    'std': self.train_data[c_col].std(),
                    'count_1': (self.train_data[c_col] == 1).sum(),
                    'count_0': (self.train_data[c_col] == 0).sum()
                }
            
            logger.info(f"è®­ç»ƒé›†Cå€¼ç»Ÿè®¡: {c_stats}")
            
            predictions = []
            
            for idx in range(n_samples):
                row_predictions = []
                
                for c_col in ['C1', 'C2', 'C3', 'C4']:
                    # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒ
                    prob_1 = c_stats[c_col]['mean']
                    
                    # æ·»åŠ ä¸€äº›éšæœºæ€§ï¼Œä½†ä¿æŒåœ¨åˆç†èŒƒå›´å†…
                    noise = np.random.normal(0, 0.05)  # å°çš„å™ªå£°
                    adjusted_prob = np.clip(prob_1 + noise, 0.1, 0.9)
                    
                    # ç”Ÿæˆé¢„æµ‹
                    prediction = 1 if np.random.random() < adjusted_prob else 0
                    row_predictions.append(prediction)
                
                predictions.append(row_predictions)
            
            result = np.array(predictions, dtype=int)
            
            # éªŒè¯é¢„æµ‹ç»“æœ
            for i, c_col in enumerate(['C1', 'C2', 'C3', 'C4']):
                pred_mean = result[:, i].mean()
                train_mean = c_stats[c_col]['mean']
                logger.info(f"{c_col} - è®­ç»ƒå‡å€¼: {train_mean:.3f}, é¢„æµ‹å‡å€¼: {pred_mean:.3f}")
            
            logger.info(f"C1-C4é¢„æµ‹å®Œæˆï¼Œå½¢çŠ¶: {result.shape}")
            return result
            
        except Exception as e:
            logger.error(f"C1-C4é¢„æµ‹å¤±è´¥: {e}")
            return np.zeros((len(test_data), 4), dtype=int)
    
    def generate_submit1(self) -> Optional[pd.DataFrame]:
        """ç”Ÿæˆsubmit1.csv
        
        Returns:
            åŒ…å«500ä¸ªADCåˆ†å­çš„DataFrameï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            logger.info("å¼€å§‹ç”Ÿæˆsubmit1.csv...")
            
            # ç”Ÿæˆå¤šæ ·æ€§åˆ†å­
            payloads = self.generate_diverse_molecules('Payload', 500)
            linkers = self.generate_diverse_molecules('Linker', 500)
            
            if len(payloads) != 500 or len(linkers) != 500:
                logger.error(f"åˆ†å­ç”Ÿæˆæ•°é‡ä¸è¶³: Payload={len(payloads)}, Linker={len(linkers)}")
                return None
            
            # æ™ºèƒ½é€‰æ‹©å…¶ä»–å­—æ®µ
            train_sample = self.train_data.sample(n=500, replace=True, random_state=self.random_seed).reset_index(drop=True)
            
            # åŸºäºè®­ç»ƒæ•°æ®ç»Ÿè®¡ç”ŸæˆC1-C4å€¼
            c_stats = {}
            for c_col in ['C1', 'C2', 'C3', 'C4']:
                c_stats[c_col] = self.train_data[c_col].mean()
            
            c_values = {}
            for c_col in ['C1', 'C2', 'C3', 'C4']:
                prob = c_stats[c_col]
                c_values[c_col] = np.random.choice([0, 1], size=500, p=[1-prob, prob]).astype(int)
            
            # æ„å»ºæ•°æ®
            submit1_data = {
                'index': range(1, 501),
                'Antibody Light Chain Sequence': train_sample['Antibody Light Chain Sequence'].tolist(),
                'Antibody Heavy Chain Sequence': train_sample['Antibody Heavy Chain Sequence'].tolist(),
                'Antigen Sequence': train_sample['Antigen Sequence'].tolist(),
                'Payload Isosmiles': payloads,
                'Linker Isosmiles': linkers,
                'DAR': train_sample['DAR'].tolist(),
                'C1': c_values['C1'].tolist(),
                'C2': c_values['C2'].tolist(),
                'C3': c_values['C3'].tolist(),
                'C4': c_values['C4'].tolist()
            }
            
            submit1_df = pd.DataFrame(submit1_data)
            
            # éªŒè¯ç”Ÿæˆçš„æ•°æ®
            if not self.validate_submit1(submit1_df):
                logger.error("submit1.csvéªŒè¯å¤±è´¥")
                return None
            
            logger.info(f"submit1.csvç”ŸæˆæˆåŠŸ: {len(submit1_df)} è¡Œ")
            return submit1_df
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆsubmit1.csvå¤±è´¥: {e}")
            return None
    
    def validate_submit1(self, df: pd.DataFrame) -> bool:
        """éªŒè¯submit1æ•°æ®
        
        Args:
            df: submit1æ•°æ®æ¡†
            
        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡
        """
        try:
            # æ£€æŸ¥è¡Œæ•°
            if len(df) != 500:
                logger.error(f"submit1è¡Œæ•°é”™è¯¯: {len(df)}, åº”ä¸º500")
                return False
            
            # æ£€æŸ¥åˆ—å
            expected_columns = set(self.required_train_columns)
            actual_columns = set(df.columns)
            if expected_columns != actual_columns:
                logger.error(f"submit1åˆ—åä¸åŒ¹é…: æœŸæœ›{expected_columns}, å®é™…{actual_columns}")
                return False
            
            # æ£€æŸ¥C1-C4å€¼
            for c_col in ['C1', 'C2', 'C3', 'C4']:
                unique_values = df[c_col].unique()
                if not set(unique_values).issubset({0, 1}):
                    logger.error(f"submit1çš„{c_col}åˆ—åŒ…å«éäºŒåˆ†ç±»å€¼: {unique_values}")
                    return False
            
            # æ£€æŸ¥indexåˆ—
            expected_index = list(range(1, 501))
            if df['index'].tolist() != expected_index:
                logger.error("submit1çš„indexåˆ—ä¸æ­£ç¡®")
                return False
            
            logger.info("submit1éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"submit1éªŒè¯å¤±è´¥: {e}")
            return False
    
    def generate_submit2(self) -> Optional[pd.DataFrame]:
        """ç”Ÿæˆsubmit2.csv
        
        Returns:
            åŒ…å«æµ‹è¯•é›†é¢„æµ‹çš„DataFrameï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            logger.info("å¼€å§‹ç”Ÿæˆsubmit2.csv...")
            
            # é¢„æµ‹C1-C4å€¼
            c_predictions = self.predict_c_values_statistical(self.test_data)
            
            if c_predictions.shape != (len(self.test_data), 4):
                logger.error(f"é¢„æµ‹ç»“æœå½¢çŠ¶é”™è¯¯: {c_predictions.shape}")
                return None
            
            # æ„å»ºæ•°æ®
            submit2_data = {
                'index': self.test_data['index'].tolist(),
                'C1': c_predictions[:, 0],
                'C2': c_predictions[:, 1],
                'C3': c_predictions[:, 2],
                'C4': c_predictions[:, 3]
            }
            
            submit2_df = pd.DataFrame(submit2_data)
            
            # éªŒè¯ç”Ÿæˆçš„æ•°æ®
            if not self.validate_submit2(submit2_df):
                logger.error("submit2.csvéªŒè¯å¤±è´¥")
                return None
            
            logger.info(f"submit2.csvç”ŸæˆæˆåŠŸ: {len(submit2_df)} è¡Œ")
            return submit2_df
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆsubmit2.csvå¤±è´¥: {e}")
            return None
    
    def validate_submit2(self, df: pd.DataFrame) -> bool:
        """éªŒè¯submit2æ•°æ®
        
        Args:
            df: submit2æ•°æ®æ¡†
            
        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡
        """
        try:
            # æ£€æŸ¥è¡Œæ•°
            if len(df) != len(self.test_data):
                logger.error(f"submit2è¡Œæ•°é”™è¯¯: {len(df)}, åº”ä¸º{len(self.test_data)}")
                return False
            
            # æ£€æŸ¥åˆ—å
            expected_columns = {'index', 'C1', 'C2', 'C3', 'C4'}
            actual_columns = set(df.columns)
            if expected_columns != actual_columns:
                logger.error(f"submit2åˆ—åä¸åŒ¹é…: æœŸæœ›{expected_columns}, å®é™…{actual_columns}")
                return False
            
            # æ£€æŸ¥C1-C4å€¼
            for c_col in ['C1', 'C2', 'C3', 'C4']:
                unique_values = df[c_col].unique()
                if not set(unique_values).issubset({0, 1}):
                    logger.error(f"submit2çš„{c_col}åˆ—åŒ…å«éäºŒåˆ†ç±»å€¼: {unique_values}")
                    return False
            
            # æ£€æŸ¥indexåˆ—
            if not df['index'].equals(self.test_data['index']):
                logger.error("submit2çš„indexåˆ—ä¸æµ‹è¯•æ•°æ®ä¸åŒ¹é…")
                return False
            
            logger.info("submit2éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"submit2éªŒè¯å¤±è´¥: {e}")
            return False
    
    def save_submission_files(self, submit1_df: pd.DataFrame, submit2_df: pd.DataFrame) -> bool:
        """ä¿å­˜æäº¤æ–‡ä»¶
        
        Args:
            submit1_df: submit1æ•°æ®
            submit2_df: submit2æ•°æ®
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("ä¿å­˜æäº¤æ–‡ä»¶...")
            
            # åˆ›å»ºsubmitç›®å½•
            submit_dir = Path("submit")
            submit_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜submit1.csv
            submit1_path = submit_dir / "submit1.csv"
            submit1_df.to_csv(submit1_path, index=False, encoding='utf-8')
            logger.info(f"submit1.csvå·²ä¿å­˜: {submit1_path}")
            
            # ä¿å­˜submit2.csv
            submit2_path = submit_dir / "submit2.csv"
            submit2_df.to_csv(submit2_path, index=False, encoding='utf-8')
            logger.info(f"submit2.csvå·²ä¿å­˜: {submit2_path}")
            
            # éªŒè¯æ–‡ä»¶
            if not self.verify_saved_files():
                logger.error("ä¿å­˜çš„æ–‡ä»¶éªŒè¯å¤±è´¥")
                return False
            
            logger.info("æäº¤æ–‡ä»¶ä¿å­˜æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æäº¤æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def verify_saved_files(self) -> bool:
        """éªŒè¯ä¿å­˜çš„æ–‡ä»¶
        
        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡
        """
        try:
            submit_dir = Path("submit")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            submit1_path = submit_dir / "submit1.csv"
            submit2_path = submit_dir / "submit2.csv"
            
            if not submit1_path.exists():
                logger.error(f"submit1.csvæ–‡ä»¶ä¸å­˜åœ¨: {submit1_path}")
                return False
            
            if not submit2_path.exists():
                logger.error(f"submit2.csvæ–‡ä»¶ä¸å­˜åœ¨: {submit2_path}")
                return False
            
            # é‡æ–°è¯»å–å¹¶éªŒè¯
            submit1_verify = pd.read_csv(submit1_path)
            submit2_verify = pd.read_csv(submit2_path)
            
            if not self.validate_submit1(submit1_verify):
                logger.error("é‡æ–°è¯»å–çš„submit1.csvéªŒè¯å¤±è´¥")
                return False
            
            if not self.validate_submit2(submit2_verify):
                logger.error("é‡æ–°è¯»å–çš„submit2.csvéªŒè¯å¤±è´¥")
                return False
            
            logger.info("ä¿å­˜çš„æ–‡ä»¶éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"éªŒè¯ä¿å­˜çš„æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def create_submission_package(self) -> bool:
        """åˆ›å»ºæäº¤å‹ç¼©åŒ…
        
        Returns:
            åˆ›å»ºæ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("åˆ›å»ºæäº¤å‹ç¼©åŒ…...")
            
            submit_dir = Path("submit")
            zip_path = Path("submit_final.zip")
            
            # åˆ é™¤å·²å­˜åœ¨çš„å‹ç¼©åŒ…
            if zip_path.exists():
                zip_path.unlink()
            
            # åˆ›å»ºå‹ç¼©åŒ…
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file_path in submit_dir.glob("*.csv"):
                    arcname = f"submit/{file_path.name}"
                    zipf.write(file_path, arcname)
                    logger.info(f"æ·»åŠ æ–‡ä»¶åˆ°å‹ç¼©åŒ…: {arcname}")
            
            # éªŒè¯å‹ç¼©åŒ…
            if not zip_path.exists():
                logger.error("å‹ç¼©åŒ…åˆ›å»ºå¤±è´¥")
                return False
            
            zip_size = zip_path.stat().st_size
            logger.info(f"æäº¤å‹ç¼©åŒ…åˆ›å»ºæˆåŠŸ: {zip_path} ({zip_size} bytes)")
            
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæäº¤å‹ç¼©åŒ…å¤±è´¥: {e}")
            return False
    
    def run(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„æäº¤æ–‡ä»¶ç”Ÿæˆæµç¨‹
        
        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        try:
            logger.info("=== å¼€å§‹ADCæäº¤æ–‡ä»¶ç”Ÿæˆæµç¨‹ ===")
            
            # 1. åŠ è½½å’ŒéªŒè¯æ•°æ®
            if not self.load_and_validate_data():
                logger.error("æ•°æ®åŠ è½½å’ŒéªŒè¯å¤±è´¥")
                return False
            
            # 2. ç”Ÿæˆsubmit1.csv
            submit1_df = self.generate_submit1()
            if submit1_df is None:
                logger.error("submit1.csvç”Ÿæˆå¤±è´¥")
                return False
            
            # 3. ç”Ÿæˆsubmit2.csv
            submit2_df = self.generate_submit2()
            if submit2_df is None:
                logger.error("submit2.csvç”Ÿæˆå¤±è´¥")
                return False
            
            # 4. ä¿å­˜æ–‡ä»¶
            if not self.save_submission_files(submit1_df, submit2_df):
                logger.error("æ–‡ä»¶ä¿å­˜å¤±è´¥")
                return False
            
            # 5. åˆ›å»ºå‹ç¼©åŒ…
            if not self.create_submission_package():
                logger.error("å‹ç¼©åŒ…åˆ›å»ºå¤±è´¥")
                return False
            
            # 6. ç”ŸæˆæŠ¥å‘Š
            self.generate_report(submit1_df, submit2_df)
            
            logger.info("=== ADCæäº¤æ–‡ä»¶ç”Ÿæˆæµç¨‹å®Œæˆ ===")
            return True
            
        except Exception as e:
            logger.error(f"æäº¤æ–‡ä»¶ç”Ÿæˆæµç¨‹å¤±è´¥: {e}")
            return False
    
    def generate_report(self, submit1_df: pd.DataFrame, submit2_df: pd.DataFrame):
        """ç”Ÿæˆç”ŸæˆæŠ¥å‘Š
        
        Args:
            submit1_df: submit1æ•°æ®
            submit2_df: submit2æ•°æ®
        """
        try:
            logger.info("\n=== ç”ŸæˆæŠ¥å‘Š ===")
            
            # Submit1ç»Ÿè®¡
            payload_diversity = self.calculate_diversity_score(submit1_df['Payload Isosmiles'].tolist())
            linker_diversity = self.calculate_diversity_score(submit1_df['Linker Isosmiles'].tolist())
            
            logger.info(f"Submit1ç»Ÿè®¡:")
            logger.info(f"  - è®°å½•æ•°: {len(submit1_df)}")
            logger.info(f"  - Payloadå¤šæ ·æ€§: {payload_diversity:.4f}")
            logger.info(f"  - Linkerå¤šæ ·æ€§: {linker_diversity:.4f}")
            
            for c_col in ['C1', 'C2', 'C3', 'C4']:
                c_mean = submit1_df[c_col].mean()
                logger.info(f"  - {c_col}å¹³å‡å€¼: {c_mean:.3f}")
            
            # Submit2ç»Ÿè®¡
            logger.info(f"Submit2ç»Ÿè®¡:")
            logger.info(f"  - è®°å½•æ•°: {len(submit2_df)}")
            
            for c_col in ['C1', 'C2', 'C3', 'C4']:
                c_mean = submit2_df[c_col].mean()
                logger.info(f"  - {c_col}å¹³å‡å€¼: {c_mean:.3f}")
            
            logger.info("=== æŠ¥å‘Šç”Ÿæˆå®Œæˆ ===")
            
        except Exception as e:
            logger.warning(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("\n" + "="*60)
        print("ADCæäº¤æ–‡ä»¶ç”Ÿæˆå™¨ - æœ€ç»ˆç‰ˆæœ¬")
        print("="*60)
        
        generator = ADCSubmissionGenerator(random_seed=42)
        success = generator.run()
        
        if success:
            print("\n" + "="*60)
            print("âœ… æäº¤æ–‡ä»¶ç”ŸæˆæˆåŠŸï¼")
            print("="*60)
            print("ç”Ÿæˆçš„æ–‡ä»¶:")
            print("ğŸ“ submit/")
            print("  â”œâ”€â”€ submit1.csv (500ä¸ªADCåˆ†å­)")
            print("  â””â”€â”€ submit2.csv (æµ‹è¯•é›†C1-C4é¢„æµ‹)")
            print("ğŸ“¦ submit_final.zip (æœ€ç»ˆæäº¤åŒ…)")
            print("ğŸ“‹ submission_generation.log (è¯¦ç»†æ—¥å¿—)")
            print("\nç‰¹ç‚¹:")
            print("âœ¨ ä¼˜åŒ–çš„åˆ†å­å¤šæ ·æ€§")
            print("ğŸ¯ åŸºäºç»Ÿè®¡çš„æ™ºèƒ½é¢„æµ‹")
            print("ğŸ” å®Œå–„çš„æ•°æ®éªŒè¯")
            print("ğŸ“Š è¯¦ç»†çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—")
            print("="*60)
            
        else:
            print("\n" + "="*60)
            print("âŒ æäº¤æ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼")
            print("="*60)
            print("è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ submission_generation.log è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()